import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42)
m=100 #Number of training examples
X_exam1=np.random.normal(70,10,m)
X_exam2=np.random.normal(70,10,m)
y=(0.3*X_exam1+0.7*X_exam2+np.random.normal(0,10,m)>80).astype(int)
X=np.column_stack((X_exam1,X_exam2))
plt.scatter(X[y==0][:,0],X[y==0][:,1],c='r',label='Not admitted')
plt.scatter(X[y==1][:,0],X[y==1][:,1],c='g',label='Admitted')
plt.xlabel('Exam 1 score')
plt.ylabel('Exam 2 score')
plt.legend()
plt.title('Student admission based on exam score')
plt.show()
#Sigmoid function
def sigmoid(z):
    return 1/(1+np.exp(-z))
z=np.linspace(-10,10,100)
sigmoid_values=sigmoid(z)
plt.figure(figsize=(8,5))
plt.plot(z,sigmoid_values,c='r')
plt.axhline()
plt.axvline()
plt.ylim(-0.3,1.1)
plt.grid()
plt.show()
#Add intercept term
X_bias=np.c_[np.ones((m,1)),X]
#Cost function
def cost_function(theta,X,y):
    h=sigmoid(X @ theta)
    epsilon=1e-5 #To avoid log(0)
    cost=-(1/m)*(y.T @np.log(h+epsilon)+(1-y).T @np.log(1-h+epsilon))
    return cost
#Gradient descent
def gradient_descent(X,y,alpha=0.01,num_iters=1000):
    theta=np.zeros(X.shape[1])
    for i in range(num_iters):
        h=sigmoid(X @theta)
        gradient=(1/m)*X.T @(h-y)
        theta-=alpha*gradient
    return theta
theta_optimal=gradient_descent(X_bias,y)
#For decision boundary
x_values=np.array([min(X[:,0]),max(X[:,0])])
y_values=-(theta_optimal[0]+theta_optimal[1]*x_values)/theta_optimal[2]

plt.scatter(X[y==0][:,0],X[y==0][:,1],c='r',label='Not admitted')
plt.scatter(X[y==1][:,0],X[y==1][:,1],c='g',label='Admitted')
plt.plot(x_values,y_values,label='Decision Boundary')
plt.xlabel('Exam 1 score')
plt.ylabel('Exam 2 score')
plt.legend()
plt.title('Student admission based on exam score')
plt.show()

#Evaluate accuracy
def predict(X,theta):
    prob=sigmoid(X @theta)
    return (prob>=0.5).astype(int)
y_pred=predict(X_bias,theta_optimal)
accuracy=np.mean(y_pred==y)*100
print(f'Training Accuracy:{accuracy:.2f}%')

    